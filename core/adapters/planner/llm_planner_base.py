from abc import ABC, abstractmethod
from typing import Dict, Tuple, List, Literal, Any
from core.system.metadata.task_graph import TaskGraph
from core.system.parser.task_assembler import assemble_tasks_with_graph
from core.system.utils.llm_response_parser import llm_response_parser
from core.system.formats.a2a import Task
from core.system.formats.a2a_part import Part
import traceback
import json


class LLMPlannerBase(ABC):
    async def parse(
        self,
        parts: List[Part],
        agent_cards: List[Dict] = [],
        task_history: List[Task] = []
    ) -> Tuple[Dict[str, Task], TaskGraph]:
        """
        ì…ë ¥ ìœ í˜•(chat, planning, task)ì— ë”°ë¼ ë‹¤ë¥¸ ê²½ë¡œë¡œ Task + Graphë¥¼ ìƒì„±í•©ë‹ˆë‹¤.
        """
        input_type = self._classify_input_type(parts)
        print(f"ğŸ” [Planner] Input type classified as: {input_type}")

        if input_type == "chat":
            task = await self._chat_response(parts)
            return {"chat_response": task}, TaskGraph()

        elif input_type == "task":
            return self._parse_task_json_direct(parts)

        else:  # planning
            return await self._run_llm_planning(parts, agent_cards, task_history)

    def _classify_input_type(self, parts: List[Part]) -> Literal["chat", "planning", "task"]:
        text = next((p.text for p in parts if p.type == "text"), "")
        if text.strip().startswith("{") and '"tasks"' in text:
            return "task"
        elif any(kw in text for kw in ["ê²€ìƒ‰", "ìš”ì•½", "ì •ë¦¬", "ì„¤ëª…", "ì´ë¯¸ì§€", "ë‚ ì”¨"]):
            return "planning"
        return "chat"

    async def _run_llm_planning(
        self,
        parts: List[Part],
        agent_cards: List[Dict],
        task_history: List[Task]
    ) -> Tuple[Dict[str, Task], TaskGraph]:
        """
        LLMì— contextë¥¼ í¬í•¨í•œ í”„ë¡¬í”„íŠ¸ë¥¼ ì „ë‹¬í•˜ê³  ê²°ê³¼ë¥¼ íŒŒì‹±í•©ë‹ˆë‹¤.
        ìµœëŒ€ 2íšŒ ì¬ì‹œë„í•˜ë©°, DAG ê²€ì¦ í¬í•¨.
        """
        user_input = next((p.text for p in parts if getattr(p, "type", None) == "text"), "")
        for attempt in range(2):
            try:
                llm_context = {
                    "parts": parts,
                    "agent_cards": agent_cards,
                    "prior_tasks": [t.dict() for t in task_history]
                }

                raw_response = await self._call_llm_with_parts(llm_context)
                parsed_tasks = self._parse_response(raw_response, user_input=user_input)

                tasks, graph = assemble_tasks_with_graph(parsed_tasks)

                if graph.has_cycle():
                    print("âŒ Invalid DAG: Cycle detected")
                    if attempt == 0:
                        print("ğŸ” Retrying...")
                        continue
                    else:
                        raise RuntimeError("DAG Cycle Error: LLM response is repetitive and invalid.")
                return tasks, graph

            except Exception as e:
                print(f"[Planner] Error occurred (attempt {attempt + 1}):", e)
                traceback.print_exc()
                if attempt == 1:
                    raise RuntimeError("LLM parsing or validation failed")

    def _parse_task_json_direct(self, parts: List[Part]) -> Tuple[Dict[str, Task], TaskGraph]:
        """
        JSON í˜•íƒœë¡œ ì§ì ‘ task ì…ë ¥ì´ ë“¤ì–´ì˜¨ ê²½ìš° íŒŒì‹± ì²˜ë¦¬
        """
        try:
            raw_json = next((p.text for p in parts if p.type == "text"), "{}")
            data = json.loads(raw_json)
            tasks, graph = assemble_tasks_with_graph(data["tasks"], data.get("graph"))
            return tasks, graph
        except Exception as e:
            raise RuntimeError(f"âŒ ì§ì ‘ ì…ë ¥ëœ JSON íŒŒì‹± ì‹¤íŒ¨: {e}")

    @abstractmethod
    async def _call_llm_with_parts(self, context: Dict[str, Any]) -> str:
        """
        LLM í˜¸ì¶œ: parts, agent_cards, prior_tasksë¥¼ í¬í•¨í•œ contextë¥¼ promptë¡œ êµ¬ì„±í•´ í˜¸ì¶œ
        """
        pass

    @abstractmethod
    async def _chat_response(self, parts: List[Part]) -> Task:
        """
        ì¼ë°˜ì ì¸ ë‹¨ë‹µ ì‘ë‹µ ì²˜ë¦¬ (ì˜ˆ: chat_response)
        """
        pass

    @abstractmethod
    async def generate_natural_language(self, task: dict) -> str:
        """
        Taskë¥¼ ìì—°ì–´ë¡œ ì¬êµ¬ì„±
        """
        pass

    def _parse_response(self, text: str, user_input: str = "") -> List[Task]:
        return llm_response_parser(text, Task, user_input=user_input)
